# Does RL give LLMs motivation?

_How GRPO style RL produces motivated models_

One of the contested points when it comes to AI risk is whether or not the current direction of LLMs will lead to models that have motivations. In this post I will outline how the current paradigm of reinforcement learning, which AI labs are [currently in the process of scaling up dramatically](https://openai.com/index/introducing-o3-and-o4-mini/?utm_source=chatgpt.com), should in principle, create models with motivations.

### What is motivation?

Firstly, I define a goal as a criterion on the state of a system's world model, and motivation as an aspect of behavior of the system that can be best compressed as "The system acts in ways that according to it's model will best lead to the criterion being fulfilled". Rocks do not have motivation, though they always seem to want to fall towards the ground, because they have no world model, and no criterion. A maze solving rat has motivation by this definition, because it has an internal model of the maze, and it has a criterion of solving the maze (to obtain the cheese) which it steers towards.

For an LLM to have motivations and goals, under this definition, it needs to have a model of it's situation, criteria on the state of this situation, and must steer towards it.

### What do base models learn?

The pretraining phase of LLMs trains the neural network to predict the next token in the sequence, where the sequences are taken from a dataset made up primarily from text on the internet. Predicting all of that text is quite a difficult task, and can be done more effectively with a predictive model that models what is "behind" the generation of the text, which is mostly, but not exclusively (LINK TO WORLDSIM? ver this), human beings. As a result, gradient descent converges to models that [maintain an internal representation of the human being(s) that most likely generated the text](https://arxiv.org/abs/2505.14685). Take this as an example of something that may have made it into the pretraining corpus: "Bob puts the box of cereal on the shelf, but it falls to the floor, bob then ". It's very difficult to complete the sentence without inferring that bob actually wants the box of cereal to be on the shelf, and so gradient descent is likely to converge on an algorithm that infers Bob's motivation from the start of the sentence, and then will use it to predict what comes after. This is a simple example, but the concept extends to much of what is written online; online political discussions are hard to predict without modelling which side each person is arguing for, and Stack Overflow is hard to predict if you don't know that the person replying is trying to help.

Because goals and motivations are a very effective way to model the text found on the internet, LLMs learn to model those aspects of humans.

### How does modern LLM RL work?

I'll be using GRPO here since that's what DeepSeek used to train their [r1 model](https://arxiv.org/abs/2501.12948). GRPO most likely is not used in the big closed-source labs today, partly because it has some pretty big theoretically flaws, but whatever they are currently using is extremely likely to be of the same form, such that the conclusions of this post aren't undermined (for now).

The first step in GRPO is that the LLM is given a task, in the form of a prompt, and then a collection of reasoning traces are "rolled out". a reasoning trace is the LLM's answer to the prompt, including all the thinking, planning, and tool use done by the model up until it gives it's final answer (or submits the code it's written) in the form of a long string of text. Multiple different instances of the same model answer the same prompt, with natural variations coming out of the indeterminism in the token sampling, and then these traces are marked based on correctness of their answer / completion of the task. Once marked, these traces take the place of the dataset, and then the LLM is trained on them, with the changes made to the weights being proportional to how correct the answers were (there is more detail here, it's worth reading [the DeepSeek-R1 paper](https://arxiv.org/abs/2501.12948) if you're interested). A slightly simplified version that is also used in LLM RL is rejection sampling: reasoning traces are generated, and those that resulted in the correct answer form a new dataset that is trained on, in the same way pretraining updates the model. Going forward I'll be mainly talking about rejection sampling because It's more simple yet still captures the most important aspect of GRPO that should create motivations.

![GRPO rollout process diagram](img/RL motivation image 1.png)

This process trains the LLM to do next token prediction on it's most successful past rollouts, in the same way it's trained to predict human text in pretraining. And as with pretraining, the LLM is incentivized to learn useful abstractions that let it more accurately predict those tokens. What are the abstractions it learns? To answer this, we have to ask; What is common to the rollouts that gave correct answers? There are some obvious object level answers, such as answering "Paris" when asked for the capital of France, then slightly more abstract traits such as writing syntactically valid code. But there are also more abstract traits that lead to success, such as being motivated to complete the task, and tenacity; the tendency to work around roadblocks relentlessly to achieve the desired end state. Text rollouts that look as if they are produced by a system motivated to complete the task, are more likely to complete the task correctly. Because this dataset of correct rollouts is best predicted using these traits, the LLM learns to model them internally, which in turn leads to those modelled traits exhibiting influence on the outputs in subsequent rollouts, bootstrapping into a process that imbues the LLM with the traits that result in it achieving the greatest performance on the tasks it's trained on.

![Bootstrapping process diagram](img/RL motivation image 2.png)

This alternating process of prediction and action blurs the lines between what is a model of something, and what is an intelligent agent. In this case, the thing "behind" the text that the neural net ends up modelling doesn't really exist yet; an idealized agent that would have produced the perfect curated answers every time. You can imagine a trillion monkeys on typewriters, and a curation mechanism that only takes the text produced that, when fed into a compiler, produces a working program that passes a particular test, and this process would produce a dataset where the best compression of it is an agent that can code well. Once a universal function approximator (such as a neural net) is trained to predict text coming from that data, and finds the best compression (a capable coding agent), it results in a "model" that simulates an agent that acts based on abstract representations with predictive power, and a mechanism that steers towards particular criterion of those representations. When you start using this model to output tokens, what is the difference between the causal system outputting the tokens, and a "true" agent?
