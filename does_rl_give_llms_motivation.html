<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Blog Post</title>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: Georgia, serif;
        background-color: #f4f4f4;
        color: #333;
        line-height: 1.6;
      }

      /* Navigation Bar */
      nav {
        background-color: #fff;
        border-bottom: 1px solid #ddd;
        padding: 1rem 2rem;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
      }

      nav ul {
        list-style: none;
        display: flex;
        gap: 2rem;
        max-width: 800px;
        margin: 0 auto;
      }

      nav a {
        text-decoration: none;
        color: #5f9b65;
        font-weight: 500;
        transition: color 0.2s;
      }

      nav a:hover {
        color: #4a7a50;
      }

      /* Main Content */
      .container {
        max-width: 800px;
        margin: 3rem auto 50vh;
        padding: 3rem;
        background-color: #fff;
        box-shadow: 0 0 10px rgba(0, 0, 0, 0.05);
      }

      h1 {
        font-size: 2.5rem;
        font-weight: 600;
        color: #000;
        margin-bottom: 0.5rem;
        line-height: 1.2;
        font-family: "Warnock Pro", "Palatino Linotype", "Book Antiqua",
          Palatino, serif;
      }

      .subtitle {
        font-size: 1.25rem;
        color: #666;
        font-style: italic;
        margin-bottom: 2rem;
        padding-bottom: 2rem;
        border-bottom: 1px solid #eee;
        font-family: "Warnock Pro", "Palatino Linotype", "Book Antiqua",
          Palatino, serif;
      }

      .content {
        font-size: 1.125rem;
        color: #333;
      }

      .content p {
        margin-bottom: 1.5rem;
      }

      .content a {
        color: #5f9b65;
        text-decoration: none;
        border-bottom: 1px solid #5f9b65;
        transition: all 0.2s;
      }

      .content a:hover {
        color: #4a7a50;
        border-bottom-color: #4a7a50;
      }

      .content h3 {
        font-size: 1.5rem;
        font-weight: 600;
        color: #000;
        margin-top: 2rem;
        margin-bottom: 1rem;
        font-family: "Warnock Pro", "Palatino Linotype", "Book Antiqua",
          Palatino, serif;
      }

      .content img {
        max-width: 100%;
        height: auto;
        margin: 2rem 0;
        display: block;
      }

      /* Responsive */
      @media (max-width: 768px) {
        .container {
          margin: 1rem;
          padding: 1.5rem;
        }

        h1 {
          font-size: 2rem;
        }

        nav ul {
          flex-direction: column;
          gap: 1rem;
        }
      }
    </style>
  </head>
  <body>
    <nav>
      <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="about.html">About</a></li>
      </ul>
    </nav>

    <div class="container">
      <h1>Does RL give LLMs motivation?</h1>
      <div class="subtitle">How GRPO style RL produces motivated models</div>

      <div class="content">
        <p>
          One of the contested points when it comes to AI risk is whether or not
          the current direction of LLM training will lead to models that have
          motivations. In this post I will outline how the current paradigm of
          reinforcement learning, which AI labs are
          <a
            href="https://openai.com/index/introducing-o3-and-o4-mini/?utm_source=chatgpt.com"
            >currently in the process of scaling up dramatically</a
          >, should in principle, create models with motivations.
        </p>

        <h3>What is motivation?</h3>

        <p>
          Firstly, I define a goal as a criterion on the state of a system's
          world model, and motivation as an aspect of behavior of the system
          that can be best compressed as "The system acts in ways that according
          to it's model will best lead to the criterion being fulfilled". Rocks
          do not have motivation, though they always seem to want to fall
          towards the ground, because they have no world model, and no
          criterion. A maze solving rat has motivation by this definition,
          because it has an internal model of the maze, and it has a criterion
          of solving the maze (to obtain the cheese) which it steers towards.
        </p>

        <p>
          For an LLM to have motivations and goals, under this definition, it
          needs to have a model of it's situation, criteria on the state of this
          situation, and must steer towards it.
        </p>

        <h3>What do base models learn?</h3>

        <p>
          The pretraining phase of LLMs trains the neural network to predict the
          next token in the sequence, where the sequences are taken from a
          dataset made up primarily from text on the internet. Predicting all of
          that text is quite a difficult task, and can be done more effectively
          with a predictive model that models what is "behind" the generation of
          the text, which is mostly, but not exclusively (LINK TO WORLDSIM? ver
          this), human beings. As a result, gradient descent converges to models
          that
          <a href="https://arxiv.org/abs/2505.14685"
            >maintain an internal representation of the human being(s) that most
            likely generated the text</a
          >. Take this as an example of something that may have made it into the
          pretraining corpus: "Bob puts the box of cereal on the shelf, but it
          falls to the floor, bob then ". It's very difficult to complete the
          sentence without inferring that bob actually wants the box of cereal
          to be on the shelf, and so gradient descent is likely to converge on
          an algorithm that infers Bob's motivation from the start of the
          sentence, and then will use it to predict what comes after. This is a
          simple example, but the concept extends to much of what is written
          online; online political discussions are hard to predict without
          modelling which side each person is arguing for, and Stack Overflow is
          hard to predict if you don't know that the person replying is trying
          to help.
        </p>

        <p>
          Because goals and motivations are a very effective way to model the
          text found on the internet, LLMs learn to model those aspects of
          humans.
        </p>

        <h3>How does modern LLM RL work?</h3>

        <p>
          I'll be using GRPO here since that's what DeepSeek used to train their
          <a href="https://arxiv.org/abs/2501.12948">r1 model</a>. GRPO most
          likely is not used in the big closed-source labs today, partly because
          it has some pretty big theoretically flaws, but whatever they are
          currently using is extremely likely to be of the same form, such that
          the conclusions of this post aren't undermined (for now).
        </p>

        <p>
          The first step in GRPO is that the LLM is given a task, in the form of
          a prompt, and then a collection of reasoning traces are "rolled out".
          a reasoning trace is the LLM's answer to the prompt, including all the
          thinking, planning, and tool use done by the model up until it gives
          it's final answer (or submits the code it's written) in the form of a
          long string of text. Multiple different instances of the same model
          answer the same prompt, with natural variations coming out of the
          indeterminism in the token sampling, and then these traces are marked
          based on correctness of their answer / completion of the task. Once
          marked, these traces take the place of the dataset, and then the LLM
          is trained on them, with the changes made to the weights being
          proportional to how correct the answers were (there is more detail
          here, it's worth reading
          <a href="https://arxiv.org/abs/2501.12948">the DeepSeek-R1 paper</a>
          if you're interested). A slightly simplified version that is also used
          in LLM RL is rejection sampling: reasoning traces are generated, and
          those that resulted in the correct answer form a new dataset that is
          trained on, in the same way pretraining updates the model. Going
          forward I'll be mainly talking about rejection sampling because It's
          more simple yet still captures the most important aspect of GRPO that
          should create motivations.
        </p>

        <img
          src="img/RL motivation image 1.png"
          alt="GRPO rollout process diagram"
        />

        <p>
          This process trains the LLM to do next token prediction on it's most
          successful past rollouts, in the same way it's trained to predict
          human text in pretraining. And as with pretraining, the LLM is
          incentivized to learn useful abstractions that let it more accurately
          predict those tokens. What are the abstractions it learns? To answer
          this, we have to ask; What is common to the rollouts that gave correct
          answers? There are some obvious object level answers, such as
          answering "Paris" when asked for the capital of France, then slightly
          more abstract traits such as writing syntactically valid code. But
          there are also more abstract traits that lead to success, such as
          being motivated to complete the task, and tenacity; the tendency to
          work around roadblocks relentlessly to achieve the required end state.
          Text rollouts that look as if they are produced by a system motivated
          to complete the task, are more likely to complete the task correctly.
          Because this dataset of correct rollouts is best predicted using these
          traits, the LLM learns to model them internally, which in turn leads
          to those modelled traits exhibiting influence on the outputs in
          subsequent rollouts, bootstrapping into a process that imbues the LLM
          with the traits that result in it achieving the greatest performance
          on the tasks it's trained on.
        </p>

        <img
          src="img/RL motivation image 2.png"
          alt="Bootstrapping process diagram"
        />

        <p>
          This alternating process of prediction and action blurs the lines
          between what is a model of something, and what is an intelligent
          agent. In this case, the thing "behind" the text that the neural net
          ends up modelling doesn't really exist yet; an idealized agent that
          would have produced the perfect curated answers every time. You can
          imagine a trillion monkeys on typewriters, and a curation mechanism
          that only takes the text produced that, when fed into a compiler,
          produces a working program that passes a particular test, and this
          process would produce a dataset where the best compression of it is an
          agent that can code well. Once a universal function approximator (such
          as a neural net) is trained to predict text coming from that data, and
          finds the best compression (a capable coding agent), it results in a
          "model" that simulates an agent that acts based on abstract
          representations with predictive power, and a mechanism that steers
          towards particular criterion of those representations. When you start
          using this model to output tokens, what is the difference between the
          causal system outputting the tokens, and a "true" agent?
        </p>
      </div>
    </div>
  </body>
</html>
